{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f275b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2479b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"TRAIN_PATH\"\n",
    "test_dir = \"TEST_PATH\"\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "868c0c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.08, 0.08)\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.15\n",
    "\n",
    "full_train_ds = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "total_size = len(full_train_ds)\n",
    "val_size = int(val_ratio * total_size)\n",
    "train_size = total_size - val_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_ds, val_ds = torch.utils.data.random_split(full_train_ds, [train_size, val_size], generator=generator)\n",
    "\n",
    "#fro training\n",
    "train_ds.dataset.transform = train_transform\n",
    "val_ds.dataset.transform = val_transform\n",
    "\n",
    "#coree testing dfata\n",
    "test_ds = datasets.ImageFolder(test_dir,val_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d7b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open (\"class_to_idx.json\", \"w\") as f:\n",
    "  json.dump(train_ds.dataset.class_to_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds)\n",
    "class_names = full_train_ds.classes\n",
    "print(len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47a82d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Display image for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0)) # Convert from (Channels, Height, Width) to (Height, Width, Channels) this is pytorch style\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "inputs, classes = next(iter(train_loader))\n",
    "\n",
    "out = torchvision.utils.make_grid(inputs[:4]) # 4 imagess\n",
    "plt.figure(figsize=(10, 5))\n",
    "imshow(out, title=[class_names[x] for x in classes[:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d374cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class SpatialSelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels,attn_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.query_conv = nn.Conv2d(in_channels, attn_channels, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, attn_channels, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1) # Added value_conv which was missing and caused `value_conv` not defined error\n",
    "\n",
    "        #learnable scaling parameter\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        \"\"\"\n",
    "        x: [B × 256 × 8 × 8]  input feature map from stage 3\n",
    "        return: out: [B × 256 × 8 × 8] attention value\n",
    "        \"\"\"\n",
    "        B,C,H,W = x.shape  # getting batchsize, channel , height and weight from x --> input from stage 3\n",
    "\n",
    "        N = H*W\n",
    "\n",
    "        # Q,K,V projection\n",
    "        Q = self.query_conv(x)  # [B × 64 × 8 × 8]\n",
    "        K = self.key_conv(x)    # [B × 64 × 8 × 8]\n",
    "        V = self.value_conv(x)  # [B × 256 × 8 × 8]\n",
    "\n",
    "        # Flatten spatial dimensions\n",
    "        Q = Q.view(B, -1, N).permute(0, 2, 1)  # [B × 64 × 64] Transpose of Q\n",
    "        K = K.view(B, -1, N)                   # [B × 64 × 64]\n",
    "        V = V.view(B, -1, N).permute(0, 2, 1)  # [B × 64 × 256]\n",
    "\n",
    "        # Attention matrix\n",
    "        attention = torch.bmm(Q, K)            # [B × 64 × 64]\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "\n",
    "        # Apply attention to V\n",
    "        out = torch.bmm(attention, V)          # [B × 64 × 256]\n",
    "        # Reshape back\n",
    "        out = out.permute(0, 2, 1).contiguous()\n",
    "        out = out.view(B, C, H, W)              # [B × 256 × 8 × 8]\n",
    "\n",
    "        #  Residual fusion\n",
    "        out = self.gamma * out + x\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Basnet(nn.Module):\n",
    "    def __init__(self, num_classes=46):\n",
    "        super().__init__()\n",
    "        self.attention = SpatialSelfAttention(in_channels=256) # Instantiate SpatialSelfAttention\n",
    "\n",
    "        #  Stem (Bridge between input image and residual block )  just increases the feature map\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        #  Residual Stages\n",
    "        self.stage1 = nn.Sequential(\n",
    "            ResidualBlock(64, 64),\n",
    "            ResidualBlock(64, 64),   #64x32x32\n",
    "        )\n",
    "\n",
    "        self.stage2 = nn.Sequential(\n",
    "            ResidualBlock(64, 128,2),\n",
    "            ResidualBlock(128, 128),   #128x16x16\n",
    "        )\n",
    "\n",
    "        self.stage3 = nn.Sequential(\n",
    "            ResidualBlock(128, 256,2),\n",
    "            ResidualBlock(256, 256),  #256x8x8\n",
    "        )\n",
    "\n",
    "        self.stage4 = nn.Sequential(\n",
    "            ResidualBlock(256, 512,2),  \n",
    "\n",
    "        )\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        #  Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.attention(x) #at 8x8 feature map\n",
    "        x = self.stage4(x)  # [B,512,4,4]\n",
    "        x = self.gap(x)    # Global Average Pooling to [B,512,1,1]\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor [B,512]\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34961704",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Basnet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3,weight_decay=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c81f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "  def __init__(self,patience=7, delta=0):\n",
    "    self.patience = patience\n",
    "    self.delta = delta\n",
    "    self.counter = 0\n",
    "    self.best_score = None\n",
    "    self.early_stop = False\n",
    "\n",
    "  def __call__(self,val_loss):\n",
    "    if self.best_score is None:\n",
    "      self.best_score = val_loss\n",
    "    elif val_loss > self.best_score + self.delta:\n",
    "      self.counter += 1\n",
    "      if self.counter >= self.patience:\n",
    "        self.early_stop = True\n",
    "    else:\n",
    "      self.best_score = val_loss\n",
    "      self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=35):\n",
    "    train_history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    stopper = EarlyStopping(patience=5)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_history[\"train_loss\"].append(avg_train_loss)\n",
    "\n",
    "        #validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = outputs.argmax(1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = correct / total\n",
    "\n",
    "        train_history[\"val_loss\"].append(avg_val_loss)\n",
    "        train_history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val ACC: {val_acc:.4f}\")\n",
    "\n",
    "        stopper(avg_val_loss)\n",
    "        if stopper.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return model, train_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model,train_history = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8084c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training visualization \n",
    "# training and validation loss visualization \n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_history['train_loss'], label=\"Train Loss\", color='orange')\n",
    "plt.plot(train_history['val_loss'], label=\"Validation Loss\", color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# validation accuracy visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_history['val_acc'], label='Val Acc')\n",
    "plt.legend()\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,test_loader, criterion):\n",
    "  model.eval()\n",
    "  test_history = {\"avg_test_loss\": [], \"test_acc\": []}\n",
    "  test_loss = 0.0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for images,labels in test_loader:\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs,labels)\n",
    "      test_loss += loss.item()\n",
    "\n",
    "      preds = outputs.argmax(dim=1)\n",
    "      correct += (preds == labels).sum().item()\n",
    "      total += labels.size(0)\n",
    "\n",
    "  test_history[\"avg_test_loss\"] = test_loss / len(test_loader)\n",
    "  test_history[\"test_acc\"] = correct / total\n",
    "\n",
    "  print(f\"Test Loss: {test_history['avg_test_loss']:.4f} | Test ACC: {test_history['test_acc']:.4f}\")\n",
    "\n",
    "  return test_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c54f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # ensure 1 channel\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5],\n",
    "        std=[0.5]\n",
    "    )\n",
    "])\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "test_history = evaluate_model(\n",
    "    model,\n",
    "    test_loader,\n",
    "    criterion\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99608d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing test loss and test accuracy \n",
    "# training and validation loss visualization\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_history['train_loss'], label=\"Train Loss\", color='blue')\n",
    "plt.plot(test_history['avg_test_loss'], label=\"Test Loss\", color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# validation accuracy visualization \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(test_history['test_acc'], label='Test Acc')\n",
    "plt.legend()\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
